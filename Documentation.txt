1. How I worked with the data

I had to very heavily clean this data in order to work with it. Most the data was
duplicated per gender and urban vs. rural, so I had to strip all of that out using
OpenRefine. In addition, I had to change various values of various countries in order
to get them to match the geo-json data- For example, Hong Kong -> Hong Kong S.A.R. or
Republic of Serbia -> Serbia.

In addition, there were many countries that I thought should be in a visualization
like this missing from the dataset. A lot of western european countries and east 
asian countries, for example. So, I researched and added estimates for those language
populations by hand.

However, once I had the data, I still had to transform it in various ways to fit my
needs, to finally get it into a form of a country that then had an array of languages
each with their own value. I also denote whether each country was originally in the
dataset or had its values generated from another source.

Despite all this, the data still has a lot of issues, mostly due to the fact that 
so many countries have so many different kinds of tabulation techniques. Some
countries had abbreviations that made their data useless, and others had values like
'Chinese and English', which was also difficult to use. I stripped a lot of the
useless data in my code though- values like 'total' or 'unknown'.

2. What you analyzed about the data in terms of data variables and how that informed 
your choice of visualization.

I talked about this a bit above, but I determined I would still be able to do the
visualization that I wanted with the data given, which is an analysis of the
populations of various language speakers in different countries. The main thing
I thought would be interesting to look at was the distributions of the biggest
languages in the world- English, Chinese, etc. And I was still able to do that, so
I just rolled with it.

3. Your design for the visualization: what channels and marks are in use and why did you choose them?

I used three different charts- A donut chart showing in the preview tooltip when 
hovering over countries, a bar chart showing more details of the data when you click
on a country, and a cloropleth that the user can make themselves by choosing a 
language. The donut and the bar showed the same information, but I used the donut
to kind of entice the user to explore around the data to see at a glance which
countries had the highest variation in languages. And then they could see the full
dataset with all its values in the detail chart. And the cloropleth simply shows
distribution.

4. What scales and axes are in use and why did you use them?

I used a ordinal color scale for the donut, because I mainly just wanted to show
variation over anything else. For the bar, I used a linear color scale to show
how far down the data you are at a given time. I also use a zoomable x-axis,
so users can see data that would normally be so small compared to the higher 
language populations.

For the cloropleth, interestingly, I used a scaleLog, because for each language
one country would often completely dominate and make it hard to differentiate
between the rest of them. So I compromise exactness in scale in favour of 
being able to more easily see the distribution. 
 
5. What other information (e.g., legends) did you use and why did you use them?

The only other data I really used was in cloropleth mode, if you mouse over a
country with the given population data, it will show you the population on the 
tooltip. Not really a visualization, but it helps because otherwise you'd have to
search potentially hundreds of languages.


6. What are the interactions in the visualization?

Users can pan and zoom around the map, highlight countries to see preview info, 
click the countries to see detailed info, zoom and pan the detail info, and search
a list of languages to create a cloropleth with.

Why did you use each one?

It's a map, so zooming and panning is a given. I explained a bit above, but the other
ones are meant to both encourage the user to explore and also allow them to see the
data in different ways by zooming in on very small population languages.

What aspect of working with data does the interaction address?


If you interact long enough, you see the sheer amount of languages available to look
at.

7. Your development process.

The absolute hardest part was 1. finding a dataset that fit my needs and 2. cleaning
said dataset. Beyond that, I had a pretty clear vision in my head of what I wanted,
so I simply worked towards that. After I had my dataset ready to go, I just had to
execute. Another tricky part was testing around different color schemes to find what
made the data pop out the most.


8. Overall project experience.
To be honest, I was kicking myself for a while for picking such a difficult to work
with dataset. It took a lot of effort to get it off the ground, especially because
I had a lot of different ideas at first on how to look at global languages.
But once I was able to get it into a manageable state, I had a lot of
fun just browsing the data myself. Some ideas came from naturally from being a user,
like the cloropleth mode hover population I mentioned before. Overall I'm pretty happy
with how it turned out. 